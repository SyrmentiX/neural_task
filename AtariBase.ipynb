{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edb1dbe-20ad-49d3-aabf-27a0438da7e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1d6d12-e770-47ce-a9eb-fb93b7a2b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gymnasium.utils.save_video import save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011e03a4-a0ea-4ce2-9f00-e09b9b4c2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU enable\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU enable')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e580a6f-a987-40ec-9864-2e296bdd9ba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120abf50-431d-4c8c-bc71-e760aa7c08b4",
   "metadata": {},
   "source": [
    "Create the environment. You can use any ATARI environment from [here](https://gymnasium.farama.org/environments/atari/), but prefer to use environments with discrete action space with fewer actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9241b5-510e-4a18-bcde-8bf3c5ac2b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {'lives': 3, 'episode_frame_number': 0, 'frame_number': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Asterix-v5', render_mode='rgb_array')\n",
    "eval_env = gym.make('ALE/Asterix-v5')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828a718-db96-42f6-890f-265163fdedb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a88e3-0524-4d4f-9bd5-eb035d80fece",
   "metadata": {},
   "source": [
    "Create a replay buffer to hold game history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a761715-1ef6-4ffd-b710-1758f292888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: int | None = None):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.done = np.zeros(max_size)\n",
    "        self.step = 0\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.len = 0\n",
    "\n",
    "        self.current_state = np.zeros((max_size, *observation_space.shape))\n",
    "        self.next_state = np.zeros((max_size, *observation_space.shape))\n",
    "        self.action = np.zeros((max_size, *action_space.shape), dtype=int)\n",
    "        self.reward = np.zeros(max_size)\n",
    "        \n",
    "    def add(self, current_observation: np.ndarray, action: int, reward: float, next_observation: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observation: environment state observed at the current step\n",
    "            action: action taken by the model\n",
    "            reward: reward received after taking the action\n",
    "            next_observation: environment state obversed after taking the action\n",
    "            done: whether the episode has ended or not\"\"\"\n",
    "\n",
    "        self.current_state[self.step] = current_observation\n",
    "        self.action[self.step] = action\n",
    "        self.reward[self.step] = reward\n",
    "        self.next_state[self.step] = next_observation\n",
    "        self.done[self.step] = done\n",
    "        self.step = (self.step + 1) % self.max_size\n",
    "        self.len = min(self.len + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, done\"\"\"\n",
    "\n",
    "        indicies = self.rng.choice(self.len, size=n_samples, replace=replace)\n",
    "        return self.current_state[indicies], self.action[indicies], self.reward[indicies], self.next_state[indicies], self.done[indicies]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "\n",
    "        self.step = self.len = 0\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observation, action, reward, next observation, done\"\"\"\n",
    "\n",
    "        return self.current_state[index], self.action[index], self.reward[index], self.next_state[index], self.done[index]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669485c5-5787-4ffa-82f2-58f6f0acc151",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62c869-195b-4323-bdb4-dd53879455c9",
   "metadata": {},
   "source": [
    "Implement your model. Most if not all ATARI environments have an image observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a572a086-e9ce-4194-8809-54a8e05477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    input_features: tuple | int, \n",
    "    features: int,\n",
    "    out_features: tuple | int,\n",
    "    blocks: int, \n",
    "    dropout: float = 0.2,\n",
    "    multiply_freq: int = 1,\n",
    "    name: str | None = None\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    inputs = x = tf.keras.layers.Input(input_features, name='Input')\n",
    "\n",
    "    for i in range(blocks):\n",
    "        x = tf.keras.layers.Conv2D(features, 3, padding='same', name=f'Conv2D_{i}')(x)\n",
    "        x = tf.keras.layers.PReLU(name=f'PReLU_{i}')(x)\n",
    "        x = tf.keras.layers.MaxPool2D((2, 2), name=f'MaxPool2D_{i}')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout, name=f'Dropout_{i}')(x)\n",
    "        \n",
    "        if multiply_freq > 0 and (i + 1) % multiply_freq == 0:\n",
    "            features *= 2\n",
    "        \n",
    "\n",
    "    x = tf.keras.layers.Flatten(name='Flatten')(x)\n",
    "    x = tf.keras.layers.Dense(out_features, name='Predict')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f6a53-ffa2-44ee-985b-a8bd921592fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44c932-378b-4bff-b11d-20b4b879de9a",
   "metadata": {},
   "source": [
    "Implement the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de070f1-c6b7-4a4a-81ae-e471f159e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def __init__(self, epsilon: float, seed: int | None = None):\n",
    "        \"\"\"Selects a random action with probability `epsilon` otherwise selects the most probably action given by the model.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon: the probability to select a random action\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def __call__(self, probabilities: np.ndarray) -> int:\n",
    "        \"\"\"Select an action given the `probabilities\n",
    "\n",
    "        Parameters:\n",
    "            probabilities: probabilities for each action\n",
    "\n",
    "        Returns:\n",
    "            index of the selected action\"\"\"\n",
    "\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.integers(probabilities.shape[-1])\n",
    "        return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cc1a3-3e45-4aac-8936-d5d45cc256f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa11b5-0ca1-4bfb-91a0-33a4f8922ae9",
   "metadata": {},
   "source": [
    "Implement interacting with the environment and storing entries to the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b333e6b3-853a-4cb7-aea3-8c501d0247d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model: tf.keras.Model, buffer: ReplayBuffer | None, env: gym.Env, max_steps: int, sampler: Sampler, observation: np.ndarray | None = None) -> np.ndarray:\n",
    "    \"\"\"Play game and record\n",
    "\n",
    "    Parameters:\n",
    "        model: the model to get actions with\n",
    "        buffer: replay buffer to store the entries to\n",
    "        env: environment to play\n",
    "        max_steps: maximal number of steps to perform\n",
    "        sampler: sampler to use to sample actions\n",
    "        observation: the observation to resume from\n",
    "\n",
    "    Returns:\n",
    "        the last observation\"\"\"\n",
    "\n",
    "    if observation is None:\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "    buffer = buffer if buffer is not None else ReplayBuffer(1)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        a = sampler(model(observation[None], training=False).numpy()[0])\n",
    "        \n",
    "        new_observation, score, done, terminated, _ = env.step(a)\n",
    "        \n",
    "        buffer.add(observation, a, score, new_observation, done)\n",
    "\n",
    "        if done or terminated:\n",
    "            observation, _ = env.reset()\n",
    "            continue\n",
    "            \n",
    "        observation = new_observation\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ad3a7-0487-49d3-9ca4-6b6bbc3fa450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbeef8-70fb-44ae-92e9-0baea77e8f0e",
   "metadata": {},
   "source": [
    "Implement double q learning loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9775cbc-77a8-4b02-ab0a-3fbb22b3e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_loss(\n",
    "    current_observation: tf.Tensor, \n",
    "    action: tf.Tensor, \n",
    "    reward: tf.Tensor, \n",
    "    next_observation: tf.Tensor, \n",
    "    done: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    target_model: tf.keras.Model,\n",
    "    gamma: float\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes double q learning loss.\n",
    "\n",
    "    Parameters:\n",
    "        current_observation: observations at the current time step\n",
    "        action: actions taken at the current time step\n",
    "        reward: rewards at the current time step\n",
    "        next_observation: observations at the next time step\n",
    "        done: whether the episode has ended or not\n",
    "        model: trainig model\n",
    "        target_model: target model\n",
    "        gamma: discount\n",
    "\n",
    "    Returns:\n",
    "        Computed loss\"\"\"\n",
    "\n",
    "    q_current = model(current_observation)\n",
    "    q_next = target_model(next_observation)\n",
    "\n",
    "    a_next = tf.argmax(model(next_observation), axis=-1)\n",
    "    \n",
    "    q_ref = reward + gamma * (1. - done) * tf.reshape(tf.gather(q_next, tf.expand_dims(a_next, axis=-1), batch_dims=1), (-1, ))\n",
    "    \n",
    "    q = tf.reshape(tf.gather(q_current, tf.expand_dims(action, axis=-1), batch_dims=1), (-1, )) # Оценка от основной модели предсказанных действий\n",
    "\n",
    "    return tf.math.reduce_mean(tf.square(q_ref - q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78458a1f-054a-463a-934f-ac3f669c0e27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0aaa7-6ae5-45dc-9c52-04c474ec3ecc",
   "metadata": {},
   "source": [
    "Create models, replay buffers, sampler, optimizer, epsilon decay etc. Implement training loop, show training progress and perform model evaluation once in a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139138b8-f63c-4a2f-8793-4f96d25a353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(env.observation_space.shape, 8, 9, 3, name='Ladders', multiply_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5e187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(env.observation_space.shape, 8, 9, 3, name='Ladders', multiply_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfa51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = get_model(env.observation_space.shape, 8, 9, 3, name='target_model', multiply_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.trainable = False\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = ReplayBuffer(10000, observation_space=env.observation_space, action_space=env.action_space)\n",
    "train_sampler = Sampler(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_buffer = ReplayBuffer(100, observation_space=eval_env.observation_space, action_space=eval_env.action_space)\n",
    "eval_sampler = Sampler(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.0001, clipnorm=5, weight_decay=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fbdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 64\n",
    "decay_epochs = epochs // 2\n",
    "end_epsilon = 0.1\n",
    "update_frequency = 512\n",
    "eval_frequency = 512\n",
    "steps_per_epoch = 32\n",
    "eval_steps = 1000\n",
    "initial_samples = 1000\n",
    "n_evals = 5\n",
    "eval_threshold = 1000\n",
    "epsilon_decay = tf.keras.optimizers.schedules.PolynomialDecay(1., decay_epochs, end_learning_rate=end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0725f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "total_loss = 0\n",
    "eval_score = 0\n",
    "max_score = 0\n",
    "\n",
    "s, _ = env.reset()\n",
    "pbar = tqdm.trange(epochs)\n",
    "for i in pbar:\n",
    "    train_sampler.epsilon = epsilon_decay(i).numpy()\n",
    "    \n",
    "    s = play_game(model, train_buffer, env, steps_per_epoch, train_sampler, observation=s)\n",
    "    \n",
    "    vals = train_buffer.sample(batch_size)\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g:\n",
    "        g.watch(model.trainable_weights)\n",
    "        loss = qq_loss(*vals, model, target_model, 0.99)\n",
    "        \n",
    "    gradient = g.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_weights))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    total_loss += losses[-1]\n",
    "\n",
    "    if (i + 1) % update_frequency == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    if (i + 1) % eval_frequency == 0:\n",
    "        eval_score = 0\n",
    "\n",
    "        for i in range(n_evals):\n",
    "            eval_buffer.clear()\n",
    "            play_game(model, eval_buffer, eval_env, eval_steps, eval_sampler)\n",
    "            eval_score += eval_buffer.reward[:len(eval_buffer)].sum()\n",
    "\n",
    "        eval_score /= n_evals\n",
    "        if (int(eval_score) >= (int(max_score) + 100)):\n",
    "            max_score = eval_score\n",
    "            model.save_weights(f'models/Atari/model_weights_{int(eval_score)}.h5')\n",
    " \n",
    "        if eval_score >= eval_threshold:\n",
    "            break\n",
    "\n",
    "    pbar.set_description(f'L: {losses[-1]:.5f}; AL: {total_loss / (i + 1):.5f}; E: {eval_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65237283",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8878d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'./models/Atari/model_weights_end.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5b44e-55cd-43bb-89dd-87c28cc10a9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e5731-8d24-4482-82ed-7226fb18fa5c",
   "metadata": {},
   "source": [
    "Test the model on the environment and get a cool video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ec281c-d005-499a-b416-4d0e43437a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gameplay(model: tf.keras.Model, render_mode: str = 'human', n_frames: int = 1000, buffer_capacity: int = 1000):\n",
    "    env = gym.make('ALE/Asterix-v5', render_mode=render_mode)\n",
    "    buffer = ReplayBuffer(buffer_capacity, env.observation_space, env.action_space)\n",
    "    play_game(model, buffer, env, n_frames, Sampler(0))\n",
    "\n",
    "    if render_mode == 'rgb_array_list':\n",
    "        save_video(env.render(), './videos', durations=[1] * len(), fps=24) \n",
    "    \n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1da7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'./models/Atari/model_weights_end.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93564ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msave_gameplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m, in \u001b[0;36msave_gameplay\u001b[1;34m(model, render_mode, n_frames, buffer_capacity)\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALE/Asterix-v5\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39mrender_mode)\n\u001b[0;32m      3\u001b[0m buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(buffer_capacity, env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array_list\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      7\u001b[0m     save_video(env\u001b[38;5;241m.\u001b[39mrender(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./videos\u001b[39m\u001b[38;5;124m'\u001b[39m, durations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(), fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m) \n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m(model, buffer, env, max_steps, sampler, observation)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     21\u001b[0m     a \u001b[38;5;241m=\u001b[39m sampler(model(observation[\u001b[38;5;28;01mNone\u001b[39;00m], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     new_observation, score, done, terminated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     buffer\u001b[38;5;241m.\u001b[39madd(observation, a, score, new_observation, done)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m terminated:\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\myenv\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\myenv\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\myenv\\lib\\site-packages\\shimmy\\atari_env.py:294\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    292\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 294\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    295\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    296\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "buffer = save_gameplay(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.reward.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = save_gameplay(model, render_mode='rgb_array_list')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
