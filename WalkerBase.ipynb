{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edb1dbe-20ad-49d3-aabf-27a0438da7e0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1d6d12-e770-47ce-a9eb-fb93b7a2b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from gymnasium.utils.save_video import save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011e03a4-a0ea-4ce2-9f00-e09b9b4c2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU enable\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU enable')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e580a6f-a987-40ec-9864-2e296bdd9ba6",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120abf50-431d-4c8c-bc71-e760aa7c08b4",
   "metadata": {},
   "source": [
    "Create the [environment](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9241b5-510e-4a18-bcde-8bf3c5ac2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3', hardcore=False)\n",
    "eval_env = gym.make('BipedalWalker-v3', hardcore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122b64e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828a718-db96-42f6-890f-265163fdedb9",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a88e3-0524-4d4f-9bd5-eb035d80fece",
   "metadata": {},
   "source": [
    "Create a replay buffer to hold game history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a761715-1ef6-4ffd-b710-1758f292888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: int | None = None):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.done = np.zeros(max_size)\n",
    "        self.step = 0\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.len = 0\n",
    "\n",
    "        self.current_state = np.zeros((max_size, *observation_space.shape))\n",
    "        self.next_state = np.zeros((max_size, *observation_space.shape))\n",
    "        self.action = np.zeros((max_size, *action_space.shape), dtype=int)\n",
    "        self.reward = np.zeros(max_size)\n",
    "        \n",
    "    def add(self, current_observation: np.ndarray, action: np.ndarray, reward: float, next_observation: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observation: environment state observed at the current step\n",
    "            action: action taken by the model\n",
    "            reward: reward received after taking the action\n",
    "            next_observation: environment state obversed after taking the action\n",
    "            done: whether the episode has ended or not\"\"\"\n",
    "        self.current_state[self.step] = current_observation\n",
    "        self.action[self.step] = action\n",
    "        self.reward[self.step] = reward\n",
    "        self.next_state[self.step] = next_observation\n",
    "        self.done[self.step] = done\n",
    "        self.step = (self.step + 1) % self.max_size\n",
    "        self.len = min(self.len + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, done\"\"\"\n",
    "        indicies = self.rng.choice(self.len, size=n_samples, replace=replace)\n",
    "        return self.current_state[indicies], self.action[indicies], self.reward[indicies], self.next_state[indicies], self.done[indicies]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "        self.step = self.len = 0\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observation, action, reward, next observation, done\"\"\"\n",
    "        return self.current_state[index], self.action[index], self.reward[index], self.next_state[index], self.done[index]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669485c5-5787-4ffa-82f2-58f6f0acc151",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62c869-195b-4323-bdb4-dd53879455c9",
   "metadata": {},
   "source": [
    "Implement your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a572a086-e9ce-4194-8809-54a8e05477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(prefix: str | None = None, suffix: str | None = None, separator: str = '/') -> str | None:\n",
    "    return prefix and prefix + separator + suffix or suffix or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025c3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    input_features: tuple | int, \n",
    "    features: int,\n",
    "    out_features: tuple | int,\n",
    "    blocks: int, \n",
    "    activation: str | tf.keras.layers.Activation | None = 'silu',\n",
    "    dropout: float = 0.1,\n",
    "    multiply_freq: int = 1,\n",
    "    kind_of_model: str | None = None,\n",
    "    name: str | None = None\n",
    ") -> tf.keras.Model:\n",
    "    \n",
    "    inputs = x = tf.keras.layers.Input((input_features, ), name='input')\n",
    "    if kind_of_model != 'policy':\n",
    "        action_input = tf.keras.layers.Input((4, ), name='action_input')\n",
    "        x = tf.keras.layers.concatenate([inputs, action_input])\n",
    "        inputs = [inputs, action_input]\n",
    "\n",
    "    for i in range(blocks):\n",
    "        x = tf.keras.layers.Dense(features, activation=activation, name=f'dense_{i}')(x)\n",
    "        \n",
    "        if dropout > 0.:\n",
    "            x = tf.keras.layers.Dropout(dropout, name=f'dropout_{i}')(x)\n",
    "\n",
    "        if multiply_freq > 0 and (i + 1) % multiply_freq == 0:\n",
    "            features *= 2\n",
    "\n",
    "    out_activation = 'tanh' if kind_of_model == 'policy' else None\n",
    "    x = tf.keras.layers.Dense(out_features, activation=out_activation, name='prediction')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cc1a3-3e45-4aac-8936-d5d45cc256f1",
   "metadata": {},
   "source": [
    "# Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa11b5-0ca1-4bfb-91a0-33a4f8922ae9",
   "metadata": {},
   "source": [
    "Implement interacting with the environment and storing entries to the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b333e6b3-853a-4cb7-aea3-8c501d0247d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model: tf.keras.Model, buffer: ReplayBuffer | None, env: gym.Env, max_steps: int, observation: np.ndarray | None = None) -> np.ndarray:\n",
    "    \"\"\"Play game and record\n",
    "\n",
    "    Parameters:\n",
    "        model: the model to get actions with\n",
    "        buffer: replay buffer to store the entries to\n",
    "        env: environment to play\n",
    "        max_steps: maximal number of steps to perform\n",
    "        observation: the observation to resume from\n",
    "\n",
    "    Returns:\n",
    "        the last observation\"\"\"\n",
    "    if observation is None:\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "    buffer = buffer if buffer is not None else ReplayBuffer(1)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        a = model(observation[None], training=False).numpy()[0]\n",
    "        \n",
    "        new_observation, score, done, terminated, _ = env.step(a)\n",
    "        \n",
    "        buffer.add(observation, a, score, new_observation, done)\n",
    "\n",
    "        if done or terminated:\n",
    "            observation, _ = env.reset()\n",
    "            continue\n",
    "            \n",
    "        observation = new_observation\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ad3a7-0487-49d3-9ca4-6b6bbc3fa450",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9775cbc-77a8-4b02-ab0a-3fbb22b3e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_loss(\n",
    "    current_observation: tf.Tensor, \n",
    "    action: tf.Tensor, \n",
    "    reward: tf.Tensor, \n",
    "    next_observation: tf.Tensor,\n",
    "    done: tf.Tensor,\n",
    "    q_model: tf.keras.Model,\n",
    "    policy_model: tf.keras.Model,\n",
    "    target_q_model: tf.keras.Model,\n",
    "    target_policy_model: tf.keras.Model,\n",
    "    gamma: float\n",
    ") -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Computes Deep Deterministic Policy Gradient.\n",
    "\n",
    "    Parameters:\n",
    "        current_observation: observations at the current time step\n",
    "        action: actions taken at the current time step\n",
    "        reward: rewards at the current time step\n",
    "        next_observation: observations at the next time step\n",
    "        done: whether the episode has ended or not\n",
    "        q_model: q-function model\n",
    "        policy_model: action prediction model\n",
    "        target_q_model: target q-function model\n",
    "        target_policy_model: target action prediction model\n",
    "        gamma: discount\n",
    "\n",
    "    Returns:\n",
    "        Computed losses for q-function and policy models\"\"\"\n",
    "\n",
    "    q_current = q_model((current_observation, action))\n",
    "    q_ref = reward + gamma * (1. - done) * target_q_model((next_observation, target_policy_model(next_observation)))\n",
    "    \n",
    "    q_loss = tf.math.reduce_mean(tf.square(q_current - q_ref))\n",
    "\n",
    "    policy_loss = -tf.math.reduce_mean(q_model((current_observation, policy_model(current_observation))))\n",
    "\n",
    "    return q_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78458a1f-054a-463a-934f-ac3f669c0e27",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0aaa7-6ae5-45dc-9c52-04c474ec3ecc",
   "metadata": {},
   "source": [
    "Create models, replay buffers, optimizer. Implement training loop, show training progress and perform model evaluation once in a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139138b8-f63c-4a2f-8793-4f96d25a353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Walker\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " action_input (InputLayer)      [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 28)           0           ['input[0][0]',                  \n",
      "                                                                  'action_input[0][0]']           \n",
      "                                                                                                  \n",
      " dense_0 (Dense)                (None, 16)           464         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_0 (Dropout)            (None, 16)           0           ['dense_0[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['dropout_0[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           544         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           1056        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           2112        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          8320        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          16512       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 256)          33024       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 256)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256)          65792       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 256)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " prediction (Dense)             (None, 1)            257         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,513\n",
      "Trainable params: 132,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(env.observation_space.shape[0], 16, 1, 10, name='Walker', dropout=0.1, multiply_freq=2, activation='swish')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d9d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = get_model(env.observation_space.shape[0], 16, 1, 10, name='movement', multiply_freq=2, activation='swish')\n",
    "target_model.trainable = False\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82a7f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"policy_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 24)]              0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 16)                400       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,220\n",
      "Trainable params: 133,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy_model = get_model(env.observation_space.shape[0], 16, 4, 10, name='policy_model', multiply_freq=2, dropout=0, kind_of_model='policy') # Предсказывает действие по состоянию среды\n",
    "policy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787e12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy_model = get_model(env.observation_space.shape[0], 16, 4, 10, name='target_policy_model', multiply_freq=2, kind_of_model='policy')\n",
    "target_policy_model.trainable = False\n",
    "target_policy_model.set_weights(policy_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc75d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = ReplayBuffer(10000, observation_space=env.observation_space, action_space=env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2eff362",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_buffer = ReplayBuffer(100, observation_space=eval_env.observation_space, action_space=eval_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c250ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4, clipnorm=5, decay=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c708e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 1024\n",
    "update_frequency = 128\n",
    "eval_frequency = 512\n",
    "steps_per_epoch = 32\n",
    "eval_steps = 1000\n",
    "initial_samples = 1000\n",
    "n_evals = 5\n",
    "eval_threshold = 400\n",
    "polyak = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f871b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulpiply_weights(model: tf.keras.Model, target_model: tf.keras.Model, number: float | int) -> list[np.ndarray]:\n",
    "    return [number * target_weights + (1. - number) * model_weights for target_weights, model_weights in zip(target_model.get_weights(), model.get_weights())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefb8c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204bac1e5ca64a6eb1a75526c1b8106d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_losses = []\n",
    "p_losses = []\n",
    "total_q_loss = 0\n",
    "total_p_loss = 0\n",
    "eval_score = 0\n",
    "max_score = 0\n",
    "\n",
    "s, _ = env.reset()\n",
    "pbar = tqdm.trange(epochs)\n",
    "for i in pbar:\n",
    "    \n",
    "    s = play_game(policy_model, train_buffer, env, steps_per_epoch, observation=s) # Select action, play and store in buffer\n",
    "    \n",
    "    vals = train_buffer.sample(batch_size) # Randomly sample a batch of transitions\n",
    "\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as q_g, tf.GradientTape(watch_accessed_variables=False) as p_g:\n",
    "        q_g.watch(model.trainable_weights)\n",
    "        p_g.watch(policy_model.trainable_weights)\n",
    "        q_loss, policy_loss = ddpg_loss(*vals, model, policy_model, target_model, target_policy_model, 0.99) # MSBE and mean score from Policy\n",
    "\n",
    "    q_gradient = q_g.gradient(q_loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(q_gradient, model.trainable_weights))\n",
    "\n",
    "    p_gradient = p_g.gradient(policy_loss, policy_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(p_gradient, policy_model.trainable_weights))\n",
    "\n",
    "    q_losses.append(q_loss.numpy())\n",
    "    p_losses.append(policy_loss.numpy())\n",
    "    \n",
    "    total_q_loss += q_losses[-1]\n",
    "    total_p_loss += p_losses[-1]\n",
    "\n",
    "    if (i + 1) % update_frequency == 0:\n",
    "        target_model.set_weights(mulpiply_weights(model, target_model, polyak))\n",
    "        target_policy_model.set_weights(mulpiply_weights(policy_model, target_policy_model, polyak))\n",
    "\n",
    "    if (i + 1) % eval_frequency == 0:\n",
    "        eval_score = 0\n",
    "\n",
    "        for i in range(n_evals):\n",
    "            eval_buffer.clear()\n",
    "            play_game(policy_model, eval_buffer, eval_env, eval_steps)\n",
    "            eval_score += eval_buffer.reward[:len(eval_buffer)].sum()\n",
    "\n",
    "        eval_score /= n_evals\n",
    "\n",
    "        if eval_score < max_score:\n",
    "            max_score = eval_score\n",
    "            model.save_weights(f'models/Walker/model_weights_{int(eval_score)}.h5')\n",
    "\n",
    "        if eval_score >= eval_threshold:\n",
    "            break\n",
    "\n",
    "    pbar.set_description(f'Qloss: {q_losses[-1]:.5f}; AllQloss: {total_q_loss / (i + 1):.5f}; Ploss: {p_losses[-1]:.5f}; AllPloss: {total_p_loss / (i + 1):.5f}; E: {eval_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b8c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'./models/Walker/walker_weights_end.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56570f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-183.45501439664474"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5b44e-55cd-43bb-89dd-87c28cc10a9c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e5731-8d24-4482-82ed-7226fb18fa5c",
   "metadata": {},
   "source": [
    "Test the model on the environment and get a cool video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ec281c-d005-499a-b416-4d0e43437a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gameplay(model: tf.keras.Model, render_mode: str = 'human', n_frames: int = 1000, buffer_capacity: int = 1000):\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=render_mode)\n",
    "    buffer = ReplayBuffer(buffer_capacity, env.observation_space, env.action_space)\n",
    "    play_game(model, buffer, env, n_frames)\n",
    "\n",
    "    if render_mode == 'rgb_array_list':\n",
    "        save_video(env.render(), './videos', durations=[1] * len(), fps=24) \n",
    "    \n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec9bccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./models/Walker/walker_weights_end.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57e4ea04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "buffer = save_gameplay(policy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e78353",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.reward.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdca8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = save_gameplay(model, render_mode='rgb_array_list')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
